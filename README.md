<div align="center">
    <img src="images/logo.png">
</div>
<p align="center">

---

![Static Badge](https://img.shields.io/badge/Repo_Status%3A-Work_in_Progress-blue?style=flat&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAC0AAAAiCAMAAAD8kqB9AAAClFBMVEUAAAD%2F%2F%2F%2FBYzTCysr2WxvyQRLnSibjUy3VZSLbQyvSjHHIjEX5%2F%2F%2F5%2B%2FvIz9D8%2F%2F%2F4WhryWRz3QBHxUR7ySh%2FxbBjsTiHwbxnnTCXVQiz96eTg4N%2Fd3dzL0tH0XDPFzc32SRv0UB%2F3WRr4XRn1SR32VBv1Uhz0TB71URz0QBDySh3%2FYAP4ZxT1YxnwXRz0ZxnvTh7vVR7yPhDxQhfrTB%2FcVCLfRSzUUCfdPCbLciPQOjLu8fHc5OXn5eT85uHj4uHY19bKt6%2F3WBv2Uhv0WS%2F1TSDyWjT0Sx33XBr3Vhv0Vxz3XRr3Xhr1Uhv0Ux30SRv0Whv3Yhn2Xhr0UhzzPg%2F2RBb0Tx7zPg%2FzPg7zUR33YxjyTh%2F7XgL6WQDxTh%2FwVB%2F3Pw%2FzPA3wTh7zYRv1aBntTCDxXhzrTx%2FwSh7oYCDxaxjuTiDmSyHsWR7rPxnoQiXnchrmehr%2B9%2FTs7%2FD%2B7unW2djQ19jm2dbHxcLKuLD3v6%2FJrKL1SRvvelv25ePBxML7aRfg7%2FbS5ezY7PDA1t32VBv2UBz0SR70QxT0QxT0QxT2VRv1VRz3Yhn1Sx34VRvyTB%2F1Sh71Rhj2Uhz2TBj3YRn3Yxn1RBb5ZBfxTB%2F2ZBn7Zgz4Zhj4QhPuWB30ZBrySR3yXhzvSyDvWRv3PxDvTR%2F0WRzyYhvyPQ32aRryahrvViDuYhnxQRPzbRrsRh%2FYXSPuchnrcxjRp5vqbEr5v6%2F0cU3wWDD849vQqqD3pI32n4b2moDfhmznlmTocVD0XjX5Vhrl5ubKtKvirZ%2FPpprTpZbWnI%2F8q3z2lnv4lHn5pXXwn27fhGrfgmjlkF%2F5klzmclPjcFD1bUn0Z0Ptd0H3bDb0VCr0WyT4WRn7YRf4PQvBMCeZAAAAs3RSTlMA%2FQj72VQnHBIPBgX%2B%2Fv78%2BKqajGI0LyolBv7%2B%2Fv79%2FPX09PTu287Cv7%2B1oqCTi394c1pONCIYFRINCv7%2B%2Fv7%2B%2Fv79%2Ffv39fHw7evp4uHS0M%2FMysXFwbq4s7Cwq6SimpGQgX59eHJoYFlRUElHRD8sGRX%2B%2Fv7%2B%2Fv7%2B%2Fv7%2B%2Fv38%2FPv6%2Bvn57u3p4%2BLd19bV0tLNzcrIwsC5trSnp6eioJ%2Bem5uVlZWRkIiDbmlcS0tEOTQjHZlkpy4AAAI7SURBVDjLYqAPyBHU0Y6NjZlvCuZxLp%2BtrR2jI5iOS7nWjX2nT1xQ5ARzuKdc3Xf%2BzDV%2Fdlyq2dt3NvHbqUhAzJ50vKWxzi0Pt1sSttRLM25bAWYbbXd0YDwVgcflEirH%2BHi2dpoBmVyhe%2BWtd7isBTLxGM4MNDyRQZKBdXtlKf%2FJCLzBwua%2BGWh4BzsDE8RoY%2FyhuGQLsz3%2F5USg0Y4yjHvCufCrZlPYLMuztYt9%2Bjk%2BXjs5Y0JRJAQ0nHFbiNthaaDRQD4RhlfdPsvHKyXHSjj%2BhfYz2xbVVtsw7tbkIqxaAmg4b3kFwmjChh%2BSgRhNGIi57iyTKS45YIRDHiAONMMvyjvw70Iz2hzOMs3kQDX8qI3UARRXc2duQJLXW4MsN%2FdS864QJiQBUb1cZHnTeQvFETwTxSNXkFxdoCdoguZ2wwDdjXDOguvqcKMLkwOEOTG8mh%2FpoysOM7xGBMrapM8SnIM1aDL8PGat4waxJFPZIe7V9fZNlcQRklYiagKB%2BqLcEJ74yqlKqklmeGLGyjjSU3laimiB%2BCrAoryVwkTMgYGHUzHIUjMDr1sNPso3BeLZcKpjMmfisEwTzErRiQpUVurXmNgm0Dtjjn7WsuRCDqAMWvj1%2BAWxsHgcdPJUnRwnst6CwVIsQ1ij28vpoAALS5Ca6mIU1aujVaINklp9s%2FMtkEQ52fKCneMN4tzDDNGcsmiCsIZzGoYLcxXVhNW1mNCFLZb6q2MrILO1%2BmZKYPMorlBlIBcAACpxj1lvNSqgAAAAAElFTkSuQmCC&labelColor=%23232D4B&color=%23E57200) &nbsp; &nbsp; &nbsp;[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) &nbsp; &nbsp; &nbsp;[![python](https://img.shields.io/badge/Python-3.11-3776AB.svg?style=flat&logo=python&logoColor=white)](https://www.python.org) &nbsp; &nbsp; &nbsp;[![tensorflow](https://img.shields.io/badge/TensorFlow-1.13.1-FF6F00.svg?style=flat&logo=tensorflow)](https://www.tensorflow.org)

# Abstract

In political discourse and geopolitical analysis, national leaders' words hold profound significance, often serving as harbingers of pivotal historical moments. From impassioned rallying cries to calls for caution, presidential speeches preceding major conflicts encapsulate the multifaceted dynamics of decision-making at the apex of governance. This project aims to use deep learning techniques to decode the subtle nuances and underlying patterns of US presidential rhetoric that may signal US involvement in major wars. While accurate classification is desirable, we seek to take a step further and identify discriminative features between the two classes (i.e., interpretable learning).

Through an interdisciplinary fusion of machine learning and historical inquiry, we aspire to unearth insights into the predictive capacity of neural networks in discerning the preparatory rhetoric of US presidents preceding war. Indeed, as the venerable Prussian General and military theorist Carl von Clausewitz admonishes, "War is not merely an act of policy but a true political instrument, a continuation of political intercourse carried on with other means."<sup>1</sup>

<details>
<summary><h1 style="font-size: 22px;">Report</h1></summary>


## Table of Contents

<!--ts-->
   * [Introduction](#introduction)
   * [Methods](#methods)
   * [Literature Review](#literature-review)
      * [Datset](#dataset)
      * [Modeling Experiments](#modeling-experiments)
        * [MLP](#mlp)
        * [RNN with LSTM](#rnn-with-lstm)
        * [LSTM with Attention](#lstm-with-attention)
        * [BERT](#BERT)
      * [Test Performance](#test-performance)
   * [Conclusions](#conclusions)
   * [Sources](#sources)
<!--te-->

# Motivation
<a name="introduction"></a>

We aim to shed light on the interplay between the verbiage of national leaders and the inexorable currents of history that they set in motion. In addition to probing the efficacy of deep learning and natural language processing (NLP) while navigating the challenges inherent in the analysis of protracted textual corpora, we endeavor to examine how presidential rhetoric shapes, reflects and occasionally catalyzes the nation's trajectory toward pivotal global events. We aim to gauge the impact of leaders' orations on national decisions and international relations, furnishing novel insights and fresh perspectives on matters of global import.

Moreover, this interdisciplinary approach provides valuable tools for policymakers, historians, and the wider public. Deciphering the recurrent motifs within presidential addresses holds the potential to inform prognostication or influence forthcoming events, thereby exemplifying the enduring relevance of Clausewitzian principles in conjunction with contemporary technological innovations. In doing so, it bridges age-old theories with cutting-edge methodologies, fostering a more comprehensive understanding of how leaders adeptly frame their rhetoric to galvanize support for political endeavors. While impressive accuracy warrants attention and is important for a classification task as important as ours, we seek to make our model results interpretable; deep neural networks for classification are, to most, black boxes; we plan to use interpretable learning techniques to shed insight on how/why our models predict as they do.

# Literature Review
<a name="literature-review"></a>

NLP has significantly improved in recent years, with techniques available today for handling progressively larger text documents. For longer texts, several studies show that Recurrent Neural Networks (RNN), particularly gated variants like Long Short-Term Memory (LSTM)<sup>2</sup> and Gated Recurrent Unit (GRU)<sup>3</sup>, can capture patterns while retaining important contextual information.<sup>4,5</sup> However, LSTM and GRU structures are inefficient because they conduct recurrent operations at the token level, and research suggests these structures can suffer from vanishing gradients during backpropagation when used for longer sequences.<sup>6</sup>

Nevertheless, gated architectures are instrumental in capturing sequential dependencies in text data, yet their effectiveness in handling long-term dependencies relevant to long-text modeling tasks poses a challenge that researchers have addressed using attention mechanisms.<sup>7</sup> While attention mechanisms play a crucial role in enhancing predictive accuracy and model interpretability, addressing the challenge of handling long-term dependencies in text modeling tasks has led researchers to explore transformer-based architectures as a solution.<sup>8</sup>

Google's Bidirectional Encoder Representations from Transformers (BERT)<sup>9</sup> is one example of such a model, but myriad transformer-based architectures have emerged since Google released BERT. Despite impressive performance on a wide range of NLP tasks, these models still impose a length limitation on each input sequence, which most longer text documents far exceed. BERT's self-attention mechanism, for example, can process a maximum of 512 tokens. This requires careful preprocessing to circumvent maximum sequence length limitations; for example, researchers have explored employing truncation, chunking, etc.<sup>10</sup> Other newer approaches, like BigBird and Longformer, use sparse attention mechanisms with larger maximum token limits, and others explore fine-tuning BERT to work with longer text data, including ChunkBERT and BERT For Longer Texts (BELT).<sup>11,12</sup>

Research has shown that the structure of the BERT-based gated approaches, which use a fully connected encoding unit and apply the gate mechanism to update state memory, are computationally inefficient given the quadratic time complexity of applying self-attention in long-text modeling. A recent paper proposes addressing these issues using what the authors refer to as a Recurrent Attention Network (RAN).<sup>13</sup> The RAN model uses positional multi-head self-attention on local windows for dependency extraction and employs a Global Perception Cell (GPC) vector to propagate information across windows, concatenated with tokens in subsequent windows. The GPC vector acts as a window-level contextual representation and maintains long-distance memory, enhancing local and global understanding. Additionally, a memory review mechanism allows the GPC vector from the last window to serve as a document-level representation for classification tasks.

When it comes to interpretable learning, however, recent research suggests this technique may not provide much in terms of interpretability.<sup>14</sup> Researchers recently developed an approach called ProtoryNet, which makes predictions by finding the most similar prototype for each sentence in a sequence and feeding an RNN backbone with the proximity of each sentence of the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which the authors refer to as 'prototype trajectories.' These trajectories enable intuitive, find-grained interpretation of the RNN model's reasoning process.<sup>15</sup>

# Methods
<a name="methods"></a>

## Dataset
<a name="dataset"></a>

The data for this project comes from Joseph Lilleberg's Kaggle dataset, "United States Presidential Speeches," which Lilleberg scraped from The Miller Center at the University of Virginia.<sup>16</sup> We added a column to the Kaggle dataset that represents our binary categorical response variable (War), indicating whether the US entered a major war within one year of the president's speech. If the US entered a major war within one year of the president's speech, then the observation's value for the War variable is 1; if the US did not enter a major war within one year of the president's speech, that observation gets a 0 value for the War variable. We derived wars' start dates from the US Congressional Research Service.<sup>17</sup> 

This dataset provides a robust framework for a comprehensive exploration of presidential rhetoric. Including the 'Party' variable allows us to examine whether patterns exist in political affiliation and the content and tone of presidential speeches.

We perform some slight cleaning and preprocessing to set up the data for modeling. First, we checked for null values and found one missing transcript for a speech delivered by Thomas Jefferson on Nov. 8, 1808; we found the transcript via the Miller Center and added it to the dataset. Next, because the first war we consider (First Barbary War) started in 1801, we filter the dataset to speeches dated after 1800.

Several transcripts end with the president's signature; we remove the signature text from the transcripts column given that the president is identifiable from the president column and that text is not important for our modeling purposes. The transcripts also contain instances of long integers and floating point numbers when a president describes various treasury and debt statistics, for example. We remove floating point numbers and integers from the transcripts. Additionally, we convert the transcripts to lowercase and remove punctuation.

After cleaning the data and adding our response variable, the dataset contains 964 observations and exhibits significant class imbalance. There are 883 observations classified as War = 0 and 81 observations classified as War = 1; roughly 92% of the speeches were not delivered within one year of the US entering a major war. We use the Synthetic Minority Over-sampling Technique (SMOTE) to balance the classes, and, as the authors suggest, we combine SMOTE with random undersampling of the majority class.<sup>18</sup> We combine these transformations into a single pipeline.

## Experiments
<a name="modeling-experiments"></a>

TBD

### MLP
<a name="mlp"></a>

TBD

<div align="center">
    <img src="images/mlp.png">
</div>
<p align="center">

### RNN with LSTM
<a name="rnn-with-lstm"></a>

TBD

<div align="center">
    <img src="images/rnn_lstm.png">
</div>
<p align="center">

### LSTM with Attention
<a name="lstm-with-attention"></a>

TBD

<div align="center">
    <img src="images/lstm_attn.png">
</div>
<p align="center">

### BERT
<a name="BERT"></a>

TBD

<div align="center">
    <img src="images/bert.png">
</div>
<p align="center">

## Test Performance
<a name="test-performance"></a>

<div align="center">
    <img src="images/model_comp.png">
</div>
<p align="center">


# Conclusions
<a name="conclusions"></a>

TBD

<details>
<summary><h2 style="font-size: 18px;">Sources</h2></summary>
<a name="sources"></a>

[1]: von Clausewitz, C. (1997). On War (J. J. Graham, Trans.). Wordsworth Editions.

[2]: Sepp Hochreiter and Jurgen Schmidhuber. (1997). Long Short-Term Memory. Neural Computation 9, no. 8, pp. 1735-1780. See also Hasim Sak et al. (2014). Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition. ArXiv 1402.1128v1.

[3]: Kyunghyun Cho et al. (2014). Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pp. 1724-1734.

[4]: Zhou, Chunting, Chonglin Sun, Zhiyuan Liu and F. Lau. (2015). A C-LSTM Neural Network for Text Classification. ArXiv abs/1511.08630.

[5]: Hassan, Abdalraouf and Ausif Mahmood. (2018). Convolutional Recurrent Deep Learning Model for Sentence Classification. IEEE Access 6, pp. 13949-13957.

[6]: DeLesley Hutchins et al. (2022). Block-Recurrent Transformers. ArXiv 2203.07852v3.

[7]: Dzmitry Bahdanau et al. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. ArXiv 1409.0473.

[8]: Ashish Vaswani et al. (2017). Attention is All You Need. Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 6000-6010.

[9]: Jacob Devlin et al. (2019). BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tchnologies 1.

[10]: Zican Dong et al. (2022). A Survey on Long Text Modeling with Transformers. ArXiv 2302.14502v1. See also Park et al. (2022). Efficient Classification of Long Documents Using Transformers. ArXiv 2203.11258v1.

[11]: Aman Jaiswal and Evangelos Milios. (2023). Breaking the Token Barrier: Chunking and Convolution for Efficient Longer Text Classification with BERT. ArXiv 2310.2055av1.

[12]: Michal Brzozowski. (2023). Fine-tuning BERT model for arbitrarily long texts Part 1. MIM AI. https://www.mim.ai/fine-tuning-bert-model-for-arbitrarily-long-texts-part-1/. See also Michal Brzozowski. (2023). Fine-tuning BERT model for arbitrarily long texts, Part 2. MIM AI. https://www.mim.ai/fine-tuning-bert-model-for-arbitrarily-long-texts-part-2/. For technical documentation, see Michal Brzozowski and Marek Wachnicki. (2023). Welcome to BELT (BERT For Longer Texts)'s documentation. MIM AI. https://mim-solutions.github.io/bert_for_longer_texts/. 

[13]: Xianming Li et al. (2023). Recurrent Attention Networks for Long-text Modeling. Findings of the Association for Computational Linguistics (ACL), pp. 3006-3019.

[14]: Sarthak Jain and Byron C. Wallace. (2019). Attention is not Explanation. ArXiv 1902.10186v3.

[15]: Dat Hong et al. (2023). ProtoryNet - Interpretable Text Classification Via Prototype Trajectories. Journal of Machine Learning Research 24, pp. 1-39.

[16]: Lilleberg, J. (2020). United States presidential speeches. Kaggle. https://www.kaggle.com/datasets/littleotter/united-states-presidential-speeches; Data scraped from The Miller Center at the University of Virginia, https://millercenter.org/the-presidency/presidential-speeches.

[17]: Barbara Salazar Torreon and Carly A. Miller, US Congressional Research Service. (2024). U.S. Periods of War and Dates of Recent Conflicts, available at https://sgp.fas.org/crs/natsec/RS21405.pdf.

[18]: Nitesh V. Chawla et al. (2002). SMOTE: Synthetic Minority Over-sampling Technique. Journal of Artificial Intelligence 16, pp. 321-357.

</details>
</details>
<details>
<summary><h1 style="font-size: 22px;">Manifest</h1></summary>
  
<details>
<summary>Python Module Files (helper functions, classes)</summary>
  
### [`BertSeqVect.py`](https://github.com/WD-Scott/WordsofWar/blob/main/Python_Modules/BertSeqVect.py)

This Python module file includes the `BertSequenceVectorizer` class, which we designed to convert input text into vector representations using a pre-trained the Bidirectional Encoder Representations from Transformers (BERT) model.

  * Features:
    
    **BERT-based Vectorization**: Utilizes a pre-trained BERT model to generate vector representations of input text.
    
    **Tokenization**: Employs the BERT tokenizer to tokenize input text before vectorization.
    
    **Customizable Sequence Length**: Allows customization of the maximum length of input sequences for vectorization.
 
  * Usage
    
    Upon instantiation of the `BertSequenceVectorizer` object, the class automatically loads a pre-trained BERT model (bert-base-uncased by default) and its corresponding tokenizer, specifying the maximum length of input sequences for vectorization.

### [`plot_history.py`](https://github.com/WD-Scott/WordsofWar/blob/main/Python_Modules/plot_history.py)

This Python module file contains a helper function for plotting model history (accuracy, validation accuracy, loss, and validation loss).
    
</details>
<br>
<details>
<summary>Jupyter Notebooks</summary>

### [`Cleaning_Data.ipynb`](https://github.com/WD-Scott/WordsofWar/blob/main/Jupyter_Notebooks/Cleaning_Data.ipynb)

The Jupyter Notebook contains the code we used to clean the input data (speeches.csv) and set up the training, testing, and validation sets. In this notebook, we use the pre-trained BERT model and vectorizer (see BertSeqVect.py) to tokenize and vectorize the text data.

### [`EDA.ipynb`](https://github.com/WD-Scott/WordsofWar/blob/main/Jupyter_Notebooks/EDA.ipynb)

This Jupyter Notebook contains code and visualizations from our exploratory data analysis.

### [`Modeling.ipynb`](https://github.com/WD-Scott/WordsofWar/blob/main/Jupyter_Notebooks/Modeling.ipynb)

This Jupyter Notebook contains our code for the modeling experiments. We experiment with three models: (1) MLP, (2) gated RNN (LSTM), (3) the same second model but with Attention mechanisms, and (4) a pre-trained BERT model. After developing these models, we use the third approach to begin exploring various ways to perform interpretable learning to discern how the model differentiates the two classes.
</details>
<br>
<details>
<summary>Data Files</summary>

### [`Speeches_War_Clean.csv`](https://github.com/WD-Scott/WordsofWar/blob/main/Data_Files/Speeches_War_Clean.csv)

This file contains the cleaned data that we use for modeling.

### [`presidential_speeches.csv`](https://github.com/WD-Scott/WordsofWar/blob/main/Data_Files/presidential_speeches.csv)

This file contains the original source data.

### [`X_test.csv`](https://github.com/WD-Scott/WordsofWar/blob/main/Data_Files/X_test.csv)

This file contains the testing features (the vector representations of the input text).

### [`X_train.csv`](https://github.com/WD-Scott/WordsofWar/blob/main/Data_Files/X_train.csv)

This file contains the training features (the vector representations of the input text).

### [`X_val.csv`](https://github.com/WD-Scott/WordsofWar/blob/main/Data_Files/X_val.csv)

This file contains the validation features (the vector representations of the input text).

### [`y_test.csv`](https://github.com/WD-Scott/WordsofWar/blob/main/Data_Files/y_test.csv)

This file contains the testing labels (binary response variable 'War').

### [`y_train.csv`](https://github.com/WD-Scott/WordsofWar/blob/main/Data_Files/y_train.csv)

This file contains the training labels (binary response variable 'War').

### [`y_val.csv`](https://github.com/WD-Scott/WordsofWar/blob/main/Data_Files/y_val.csv)

This file contains the validation labels (binary response variable 'War').
</details>
</details>
